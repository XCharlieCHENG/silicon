{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SILICON Demo Toolkit: Fearspeech Task\n",
        "\n",
        "This notebook is part of the SILICON toolkit released by Xiang Cheng, Raveesh Mayya, and João Sedoc: “To Err Is Human; To Annotate, SILICON? Reducing Measurement Error in LLM Annotation.” See the arXiv preprint: [link](https://arxiv.org/abs/2412.14461).\n",
        "\n",
        "- **Dataset provenance**: The demo dataset and annotation guidelines are based on Saha et al. (2023), “On the rise of fear speech in online social media,” Proceedings of the National Academy of Sciences 120(11): e2212270120. The annotation task is to identify fearspeech and hatespeech in social media posts. The \"majority_label\" column is considered as the human annotation baseline, provided by the original dataset.\n",
        "- **Demo note**: Under `silicon_demo/outputs/`, several pre-saved files correspond to a small demonstration subset (~10 rows) to help you run the code.\n",
        "\n",
        "### What this notebook does\n",
        "- **Section 0**: Set up paths, inputs, and prompts for the fearspeech task.\n",
        "- **Section 1**: Run multiple LLMs (OpenAI, Anthropic, Google) to annotate a sample of items.\n",
        "- **Section 2**: Merge multiple runs into a single combined CSV.\n",
        "- **Section 3**: Compute agreement (Cohen’s Kappa) between ground truth and LLM labels.\n",
        "- **Section 4**: Build a regression-ready CSV and (optionally) run an R-based comparison script.\n",
        "- **Section 5**: Generate an FSD-gated threshold plot showing how agreement changes as we vary the threshold for using a single model vs. majority vote.\n",
        "\n",
        "If you only want to explore the analysis and plots, you can skip the inference step if you already have combined CSVs in `silicon_demo/outputs/Combined_Files/`. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 0 — Setup paths, inputs, and prompts\n",
        "\n",
        "Key variables:\n",
        "- **`SCRIPT_DIR`**: Absolute path to this `silicon_demo` folder.\n",
        "- **`BASE_PATH`**: Base path for reading/writing demo artifacts (defaults to `SCRIPT_DIR`).\n",
        "- **`OUTPUT_DIR`**: Where combined outputs are saved (defaults to `outputs/Combined_Files`).\n",
        "- **`FEARSPEECH_DATA_PATH`**: Path to the demo CSV (defaults to `inputs/pnas_2023_fearspeech_sample.csv`).\n",
        "- **`pattern_label`**: Regex for extracting `id` and `label` from LLM responses.\n",
        "- **`task_prompts`**: Instruction and user prompts loaded from `inputs/prompt_fearspeech.json`.\n",
        "\n",
        "If you are reusing pre-saved outputs under `silicon_demo/outputs/`, you may skip running Section 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SCRIPT_DIR: /Users/xccheng/Library/CloudStorage/GoogleDrive-xccheng@umd.edu/.shortcut-targets-by-id/11QVTbCjNL67dKiCLHtVOQl6q-5xE5NuJ/LLMs_for_Business_School_Research_Annotations/silicon_demo\n",
            "BASE_PATH:  /Users/xccheng/Library/CloudStorage/GoogleDrive-xccheng@umd.edu/.shortcut-targets-by-id/11QVTbCjNL67dKiCLHtVOQl6q-5xE5NuJ/LLMs_for_Business_School_Research_Annotations/silicon_demo\n",
            "Loaded sample with 160 rows; columns: ['text', 'id', 'class1', 'class2', 'class3', 'worker1', 'worker2', 'worker3', 'majority_label', 'agree_strict', 'agree', 'label_3times', 'worker_amt1', 'worker_amt2', 'worker_amt3']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "\n",
        "# Resolve paths relative to this demo folder\n",
        "SCRIPT_DIR = os.path.dirname(os.path.abspath(\"__file__\")) if \"__file__\" in globals() else os.getcwd()\n",
        "REPO_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, '..'))\n",
        "BASE_PATH = os.getenv('BASE_PATH', SCRIPT_DIR)\n",
        "OUTPUT_DIR = os.getenv('OUTPUT_DIR', 'outputs/Combined_Files')\n",
        "\n",
        "# Paths and inputs\n",
        "DATA_PATH = os.getenv('FEARSPEECH_DATA_PATH', os.path.join(SCRIPT_DIR, 'inputs', 'pnas_2023_fearspeech_sample.csv'))\n",
        "df_task = pd.read_csv(DATA_PATH)\n",
        "pattern_label = r'\"id\":\\s*\"([^\"]+)\"\\s*,\\s*\"label\":\\s*\"([^\"]+)\"'\n",
        "with open(os.path.join(SCRIPT_DIR, 'inputs', 'prompt_fearspeech.json'), 'r', encoding='utf-8') as json_file:\n",
        "    task_prompts = json.load(json_file)\n",
        "\n",
        "print(f\"SCRIPT_DIR: {SCRIPT_DIR}\")\n",
        "print(f\"BASE_PATH:  {BASE_PATH}\")\n",
        "print(f\"Loaded sample with {len(df_task)} rows; columns: {list(df_task.columns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 1 — LLM inference for fearspeech\n",
        "\n",
        "We instantiate API clients (if keys are provided) and call a helper `run_llm` for each model:\n",
        "- **What `run_llm` does** (from `scr/run_llm_func.py`):\n",
        "  - Builds per-row prompts using the instruction and user prompt beginning from `inputs/prompt_fearspeech.json`.\n",
        "  - Calls the selected model family (OpenAI GPT, Anthropic Claude, Google Gemini) via thin wrappers to obtain the model’s output and, when available, logprob metadata and token counts.\n",
        "  - Writes a temporary CSV under `silicon_demo/wip/` and a final iteration CSV under `silicon_demo/outputs/fearspeech/` with a `label_llm` column parsed from the model’s JSON-ish response using `pattern_fearspeech`.\n",
        "- **Why multiple iterations?** Each run is saved with a model-specific name and optional `repeat#` suffix. These can later be merged (Section 2).\n",
        "- **Tip**: You can comment out models you don’t plan to use to save time and cost. If no API keys are available, the cell just prints a message and continues.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No API clients configured. Skipping LLM inference (set OPENAI_API_KEY or API_KEYS_JSON_PATH).\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "from google import genai\n",
        "import anthropic\n",
        "from scr.run_llm_func import run_llm\n",
        "\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY')\n",
        "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
        "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')\n",
        "\n",
        "client_openai = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None\n",
        "client_deepseek = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=\"https://api.deepseek.com\") if DEEPSEEK_API_KEY else None\n",
        "client_gemini = genai.Client(api_key=GEMINI_API_KEY) if GEMINI_API_KEY else None\n",
        "client_claude = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY) if ANTHROPIC_API_KEY else None\n",
        "\n",
        "if not any([client_openai, client_deepseek, client_gemini, client_claude]):\n",
        "    print(\"No API clients configured. Skipping LLM inference (set OPENAI_API_KEY or API_KEYS_JSON_PATH).\")\n",
        "\n",
        "# Run LLMs for task\n",
        "if client_openai is not None:\n",
        "    _ = run_llm(client_openai, df_task, model='gpt-4o',\n",
        "            instruction_prompt=task_prompts['instruction_prompt'],\n",
        "            user_prompt_beginning=task_prompts['user_prompt_beginning'],\n",
        "            base_path=BASE_PATH,\n",
        "            pattern=pattern_label, model_series='gpt')\n",
        "    _ = run_llm(client_openai, df_task, model='gpt-4.1-2025-04-14',\n",
        "            instruction_prompt=task_prompts['instruction_prompt'],\n",
        "            user_prompt_beginning=task_prompts['user_prompt_beginning'],\n",
        "            base_path=BASE_PATH,\n",
        "            pattern=pattern_label, model_series='gpt')\n",
        "    _ = run_llm(client_openai, df_task, model='o3-mini',\n",
        "            instruction_prompt=task_prompts['instruction_prompt'],\n",
        "            user_prompt_beginning=task_prompts['user_prompt_beginning'],\n",
        "            base_path=BASE_PATH,\n",
        "            pattern=pattern_label, model_series='gpt')\n",
        "if client_claude is not None:\n",
        "    _ = run_llm(client_claude, df_task, model='claude-3-7-sonnet-20250219',\n",
        "            instruction_prompt=task_prompts['instruction_prompt'],\n",
        "            user_prompt_beginning=task_prompts['user_prompt_beginning'],\n",
        "            base_path=BASE_PATH,\n",
        "            pattern=pattern_label, model_series='claude')\n",
        "if client_gemini is not None:\n",
        "    _ = run_llm(client_gemini, df_task, model='gemini-2.5-pro-preview-03-25',\n",
        "            instruction_prompt=task_prompts['instruction_prompt'],\n",
        "            user_prompt_beginning=task_prompts['user_prompt_beginning'],\n",
        "            base_path=BASE_PATH,\n",
        "            pattern=pattern_label, model_series='gemini')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 2 — Merge multiple iteration outputs\n",
        "\n",
        "We consolidate per-model iteration CSVs into a single combined file using `merge_multiple_runs` from `scr/run_llm_func.py`:\n",
        "- Scans `outputs/fearspeech/` for files named like `iteration_<model>[_repeat#].csv`.\n",
        "- Renames each `label_llm` column with a unique suffix (e.g., `label_llmgpt-4o`, `label_llmo3-mini`, `label_llmgpt-4o_repeat1`).\n",
        "- Merges on `id` and preserves `['id','text','majority_label']` when present.\n",
        "- Writes the result to `outputs/Combined_Files/fearspeech.csv`.\n",
        "\n",
        "If no iteration files exist, it returns the existing combined CSV if already present; otherwise an empty DataFrame with the kept columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined CSV ready: /Users/xccheng/Library/CloudStorage/GoogleDrive-xccheng@umd.edu/.shortcut-targets-by-id/11QVTbCjNL67dKiCLHtVOQl6q-5xE5NuJ/LLMs_for_Business_School_Research_Annotations/silicon_demo/outputs/Combined_Files/fearspeech.csv (10 rows)\n"
          ]
        }
      ],
      "source": [
        "from scr.run_llm_func import merge_multiple_runs\n",
        "\n",
        "df = merge_multiple_runs(\n",
        "        base_path=BASE_PATH,\n",
        "        relative_path=\"outputs/fearspeech\",\n",
        "        file_pattern=r\"iteration_(.+?)\\.csv$\",\n",
        "        iteration_cols=['label_llm'],\n",
        "        key=\"fearspeech\",\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        keep_cols=['id', 'text', 'majority_label']\n",
        ")\n",
        "_combined_after_merge = os.path.join(BASE_PATH, 'outputs/Combined_Files', 'fearspeech.csv')\n",
        "if os.path.exists(_combined_after_merge):\n",
        "    try:\n",
        "        _dfc = pd.read_csv(_combined_after_merge)\n",
        "        print(f\"Combined CSV ready: {_combined_after_merge} ({len(_dfc)} rows)\")\n",
        "    except Exception as e:\n",
        "        print(f\"Note: combined CSV exists but could not be read: {e}\")\n",
        "else:\n",
        "    print(\"No combined CSV yet (run inference or place combined file under outputs/Combined_Files).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>id.1</th>\n",
              "      <th>text</th>\n",
              "      <th>majority_label</th>\n",
              "      <th>label_llmgpt_4_1_2025_04_14</th>\n",
              "      <th>label_llmgpt_4_1_2025_04_14_repeat1</th>\n",
              "      <th>label_llmgpt_4o</th>\n",
              "      <th>label_llmgpt_4o_repeat1</th>\n",
              "      <th>label_llmgpt_4o_repeat2</th>\n",
              "      <th>label_llmgpt_4o_repeat3</th>\n",
              "      <th>label_llmgpt_4o_repeat4</th>\n",
              "      <th>label_llmo3-mini_repeat1</th>\n",
              "      <th>label_llmo3_mini</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>102580904491418892_gab</td>\n",
              "      <td>102580904491418892_gab</td>\n",
              "      <td>@4therepublicyt @spotify Well then. I block tr...</td>\n",
              "      <td>hatespeech, fearspeech</td>\n",
              "      <td>fearspeech, hatespeech</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>hatespeech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10522539055944256_gab</td>\n",
              "      <td>10522539055944256_gab</td>\n",
              "      <td>Have you shot a firearm before? A \"double tap\"...</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>hatespeech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10955875460440675_gab</td>\n",
              "      <td>10955875460440675_gab</td>\n",
              "      <td>Yes and that is a serious issue. I want to exp...</td>\n",
              "      <td>hatespeech, fearspeech</td>\n",
              "      <td>fearspeech, hatespeech</td>\n",
              "      <td>fearspeech, hatespeech</td>\n",
              "      <td>fearspeech, hatespeech</td>\n",
              "      <td>fearspeech, hatespeech</td>\n",
              "      <td>fearspeech, hatespeech</td>\n",
              "      <td>hatespeech, fearspeech</td>\n",
              "      <td>fearspeech, hatespeech</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>hatespeech</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       id                    id.1  \\\n",
              "0  102580904491418892_gab  102580904491418892_gab   \n",
              "1   10522539055944256_gab   10522539055944256_gab   \n",
              "2   10955875460440675_gab   10955875460440675_gab   \n",
              "\n",
              "                                                text          majority_label  \\\n",
              "0  @4therepublicyt @spotify Well then. I block tr...  hatespeech, fearspeech   \n",
              "1  Have you shot a firearm before? A \"double tap\"...              hatespeech   \n",
              "2  Yes and that is a serious issue. I want to exp...  hatespeech, fearspeech   \n",
              "\n",
              "  label_llmgpt_4_1_2025_04_14 label_llmgpt_4_1_2025_04_14_repeat1  \\\n",
              "0      fearspeech, hatespeech                          hatespeech   \n",
              "1                  hatespeech                          hatespeech   \n",
              "2      fearspeech, hatespeech              fearspeech, hatespeech   \n",
              "\n",
              "          label_llmgpt_4o label_llmgpt_4o_repeat1 label_llmgpt_4o_repeat2  \\\n",
              "0              hatespeech              hatespeech              hatespeech   \n",
              "1              hatespeech              hatespeech              hatespeech   \n",
              "2  fearspeech, hatespeech  fearspeech, hatespeech  fearspeech, hatespeech   \n",
              "\n",
              "  label_llmgpt_4o_repeat3 label_llmgpt_4o_repeat4 label_llmo3-mini_repeat1  \\\n",
              "0              hatespeech              hatespeech               hatespeech   \n",
              "1              hatespeech              hatespeech               hatespeech   \n",
              "2  hatespeech, fearspeech  fearspeech, hatespeech               hatespeech   \n",
              "\n",
              "  label_llmo3_mini  \n",
              "0       hatespeech  \n",
              "1       hatespeech  \n",
              "2       hatespeech  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "_dfc.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 3 — LLM Performance analysis (Kappa)\n",
        "\n",
        "We compute agreement between ground truth (`majority_label`) and each model column using `output_llm_gt_kappas` from `scr/agreement_func.py`.\n",
        "- The function cleans string labels (lowercase; remove underscores, slashes, spaces) and computes Cohen’s Kappa; with `weighted=True` it uses a MASI-style weighting under the hood.\n",
        "- We detect demo-generated columns as those that start with `label_llm` (excluding a bare `label_llm` column).\n",
        "- The result is a small table with each model and its Kappa vs. ground truth. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The following is a boolean variable that indicates whether the task is a multi-label task (i.e., whether a single item can have multiple labels).\n",
        "# It affects the computation of Kappa.\n",
        "# For our demo, an item can be both fearspeech and hatespeech, so it is a multi-label task. \n",
        "is_multi_label_task=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LLM</th>\n",
              "      <th>Kappa Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gpt-4-1-2025-04-14</td>\n",
              "      <td>0.333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gpt-4-1-2025-04-14-repeat1</td>\n",
              "      <td>0.218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>GPT-4o</td>\n",
              "      <td>0.137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>gpt-4o-repeat1</td>\n",
              "      <td>0.153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>gpt-4o-repeat2</td>\n",
              "      <td>0.255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>gpt-4o-repeat3</td>\n",
              "      <td>0.137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>gpt-4o-repeat4</td>\n",
              "      <td>0.255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>o3-mini-repeat1</td>\n",
              "      <td>0.118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>o3-mini</td>\n",
              "      <td>0.123</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          LLM  Kappa Score\n",
              "0          gpt-4-1-2025-04-14        0.333\n",
              "1  gpt-4-1-2025-04-14-repeat1        0.218\n",
              "2                      GPT-4o        0.137\n",
              "3              gpt-4o-repeat1        0.153\n",
              "4              gpt-4o-repeat2        0.255\n",
              "5              gpt-4o-repeat3        0.137\n",
              "6              gpt-4o-repeat4        0.255\n",
              "7             o3-mini-repeat1        0.118\n",
              "8                     o3-mini        0.123"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from scr.agreement_func import output_llm_gt_kappas\n",
        "from scr.datasets_config import fearspeech_dict\n",
        "\n",
        "combined_path = os.path.join(BASE_PATH, 'outputs/Combined_Files', 'fearspeech.csv')\n",
        "if os.path.exists(combined_path):\n",
        "    df_task_combined = pd.read_csv(combined_path)\n",
        "    has_gt = ('majority_label' in df_task_combined.columns) and df_task_combined['majority_label'].notna().any()\n",
        "    if len(df_task_combined) > 0 and has_gt:\n",
        "        # Use demo-generated columns only (label_llm*)\n",
        "        demo_cols = [c for c in df_task_combined.columns if c.startswith('label_llm') and c != 'label_llm']\n",
        "        if demo_cols:\n",
        "            def _pretty(name: str) -> str:\n",
        "                suff = name[len('label_llm'):].lstrip('_')\n",
        "                mapping = {\n",
        "                    'gpt_4o': 'GPT-4o',\n",
        "                    'o3_mini': 'o3-mini',\n",
        "                }\n",
        "                return mapping.get(suff, suff.replace('_', '-'))\n",
        "            demo_labels = [_pretty(c) for c in demo_cols]\n",
        "            result_task = output_llm_gt_kappas(\n",
        "                    df_task_combined,\n",
        "                    gt_col='majority_label',\n",
        "                    llm_col_list=demo_cols,\n",
        "                    llm_col_labels=demo_labels,\n",
        "                    weighted=is_multi_label_task,\n",
        "            )\n",
        "            display(result_task)\n",
        "        else:\n",
        "            print(\"Warning: No demo LLM columns (label_llm*) found. Skipping kappa analysis.\")\n",
        "    elif len(df_task_combined) > 0 and not has_gt:\n",
        "        print(\"Warning: Ground truth 'majority_label' not found or empty. Skipping kappa analysis.\")\n",
        "    else:\n",
        "        print(\"Warning: Combined fearspeech CSV is empty. Skipping kappa analysis.\")\n",
        "else:\n",
        "    print(\"Warning: Combined fearspeech CSV not found in silicon_demo/outputs/Combined_Files. Skipping kappa analysis.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 4 — Regression CSV and model comparison\n",
        "\n",
        "We prepare a binary-accuracy regression CSV and optionally invoke an R script to compare models:\n",
        "- `create_regression_csv` (from `scr/regression_utils.py`) auto-detects model columns (those starting with `label_llm`) and builds a dataset where each column is a 0/1 match to the ground truth.\n",
        "- We then call `scr/regression_analysis/run_model_comparison.sh` via `bash`. This requires R and the necessary packages installed. If R is not available, the step will be skipped with a warning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created /Users/xccheng/Library/CloudStorage/GoogleDrive-xccheng@umd.edu/.shortcut-targets-by-id/11QVTbCjNL67dKiCLHtVOQl6q-5xE5NuJ/LLMs_for_Business_School_Research_Annotations/silicon_demo/outputs/Regression/fearspeech_regression.csv with 10 rows\n",
            "\n",
            "Running model comparison...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning message:\n",
            "There was 1 warning in `mutate()`.\n",
            "ℹ In argument: `std.error = sqrt(diag(vc))`.\n",
            "Caused by warning in `sqrt()`:\n",
            "! NaNs produced \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Regression Summary (binary) ===\n",
            "# A tibble: 8 × 5\n",
            "  model                       estimate std.error  statistic p.value\n",
            "  <chr>                          <dbl>     <dbl>      <dbl>   <dbl>\n",
            "1 gpt_4_1_2025_04_14_repeat1  5.08e-16   NaN     NaN        NaN    \n",
            "2 gpt_4o                     -5.39e- 1     0.572  -9.43e- 1   0.349\n",
            "3 gpt_4o_repeat1             -5.39e- 1     0.572  -9.43e- 1   0.349\n",
            "4 gpt_4o_repeat2              4.90e-16   NaN     NaN        NaN    \n",
            "5 gpt_4o_repeat3              1.62e-15     0.744   2.17e-15   1.000\n",
            "6 gpt_4o_repeat4              3.50e-16   NaN     NaN        NaN    \n",
            "7 o3_mini                     1.96e-15   NaN     NaN        NaN    \n",
            "8 o3.mini_repeat1             1.95e-15   NaN     NaN        NaN    \n",
            "\n",
            "Saved plot to: outputs/Plots/regression/combined_model_comparison_binary.png\n",
            "\n",
            "Model comparison plot generated successfully! outputs/Plots/regression/combined_model_comparison_binary.png\n"
          ]
        }
      ],
      "source": [
        "from scr.regression_utils import create_regression_csv\n",
        "\n",
        "source_path = os.path.join(BASE_PATH, 'outputs/Combined_Files', 'fearspeech.csv')\n",
        "output_path = os.path.join(BASE_PATH, 'outputs/Regression', 'fearspeech_regression.csv')\n",
        "if os.path.exists(source_path):\n",
        "    try:\n",
        "        df_src = pd.read_csv(source_path)\n",
        "        if ('majority_label' in df_src.columns) and df_src['majority_label'].notna().any():\n",
        "            create_regression_csv(source_path, 'majority_label', fearspeech_dict, output_path)\n",
        "            # Run regression analysis (binary accuracy) for the generated fearspeech CSV\n",
        "            try:\n",
        "                script_sh = os.path.join(SCRIPT_DIR, 'scr', 'regression_analysis', 'run_model_comparison.sh')\n",
        "                # Execute via bash to avoid relying on executable bit; run from demo root for relative paths\n",
        "                subprocess.run(['bash', script_sh], cwd=SCRIPT_DIR, check=True)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Regression analysis script failed: {e}\")\n",
        "        else:\n",
        "            print(\"Warning: No ground truth in combined CSV. Skipping regression CSV generation.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Failed to create regression CSV: {e}\")\n",
        "else:\n",
        "    print(\"Warning: Combined fearspeech CSV not found. Skipping regression CSV generation.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 5 — Threshold plot with FSD (sampling-based)\n",
        "\n",
        "We generate a plot of weighted Kappa vs. an FSD threshold using `plot_kappa_vs_threshold_fearspeech_with_fsd` from `scr/threshold_plot.py`:\n",
        "- **FSD (Feature Stability Distance, sampling-based here)**: Computed by `calculate_fsd` over multiple sampled outputs from the same model family (e.g., several `gpt-4o` iterations). Intuition: higher FSD means the main model is more stable/decisive on that item.\n",
        "- **Gating rule**: If FSD ≥ t, use the main model’s label; else, use a majority vote over auxiliary families. We sweep t ∈ [0, 1] and compute Kappa vs. ground truth.\n",
        "- The function reads `outputs/Combined_Files/fearspeech.csv`, identifies model columns by suffix, and saves a PNG into `silicon_demo/outputs/`.\n",
        "\n",
        "This cell also writes the per-row FSD series to `outputs/fsd_per_task/fearspeech_fsd.csv` for inspection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved sampling-based FSD series to /Users/xccheng/Library/CloudStorage/GoogleDrive-xccheng@umd.edu/.shortcut-targets-by-id/11QVTbCjNL67dKiCLHtVOQl6q-5xE5NuJ/LLMs_for_Business_School_Research_Annotations/silicon_demo/outputs/fsd_per_task/fearspeech_fsd.csv\n",
            "FSD-gated threshold plot saved: /Users/xccheng/Library/CloudStorage/GoogleDrive-xccheng@umd.edu/.shortcut-targets-by-id/11QVTbCjNL67dKiCLHtVOQl6q-5xE5NuJ/LLMs_for_Business_School_Research_Annotations/silicon_demo/outputs/kappa_vs_threshold_fearspeech_fsd.png\n",
            "Kappa vs FSD threshold:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Threshold</th>\n",
              "      <th>Kappa</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.4</td>\n",
              "      <td>0.255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.6</td>\n",
              "      <td>0.255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.7</td>\n",
              "      <td>0.255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.8</td>\n",
              "      <td>0.255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.9</td>\n",
              "      <td>0.255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.255</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Threshold  Kappa\n",
              "0         0.0  0.137\n",
              "1         0.1  0.255\n",
              "2         0.2  0.255\n",
              "3         0.3  0.255\n",
              "4         0.4  0.255\n",
              "5         0.5  0.255\n",
              "6         0.6  0.255\n",
              "7         0.7  0.255\n",
              "8         0.8  0.255\n",
              "9         0.9  0.255\n",
              "10        1.0  0.255"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from scr.threshold_plot import plot_kappa_vs_threshold_fearspeech_with_fsd\n",
        "from scr.fsd_sampling import calculate_fsd\n",
        "\n",
        "plot_save = os.path.join(SCRIPT_DIR, 'outputs', 'kappa_vs_threshold_fearspeech.png')\n",
        "plot_save_fsd = os.path.join(SCRIPT_DIR, 'outputs', 'kappa_vs_threshold_fearspeech_fsd.png')\n",
        "combined_exists = os.path.exists(os.path.join(BASE_PATH, 'outputs/Combined_Files', 'fearspeech.csv'))\n",
        "if combined_exists:\n",
        "    try:\n",
        "        df_plot_src = pd.read_csv(os.path.join(BASE_PATH, 'outputs/Combined_Files', 'fearspeech.csv'))\n",
        "        has_gt_plot = ('majority_label' in df_plot_src.columns) and df_plot_src['majority_label'].notna().any()\n",
        "        # Require demo label_llm* columns\n",
        "        has_demo_cols = any(c.startswith('label_llm') and c != 'label_llm' for c in df_plot_src.columns)\n",
        "        if has_gt_plot and has_demo_cols:\n",
        "            # Step A: Generate FSD dataset for 'gpt-4o' models (sampling-based) into silicon_demo/outputs/fsd_per_task\n",
        "            fsd_out_dir = os.path.join(SCRIPT_DIR, 'outputs', 'fsd_per_task')\n",
        "            os.makedirs(fsd_out_dir, exist_ok=True)\n",
        "            fsd_out_path = os.path.join(fsd_out_dir, 'fearspeech_fsd.csv')\n",
        "\n",
        "            # Identify 'gpt-4o*' sampled columns from combined CSV\n",
        "            def _normalize_token(s: str) -> str:\n",
        "                return ''.join(ch for ch in s.lower() if ch.isalnum())\n",
        "\n",
        "            def _match_model_cols(df_any: pd.DataFrame, family_aliases: list) -> list:\n",
        "                norm_aliases = {_normalize_token(a) for a in family_aliases}\n",
        "                cols = []\n",
        "                for c in df_any.columns:\n",
        "                    if not (c.startswith('label_llm') and c != 'label_llm'):\n",
        "                        continue\n",
        "                    suffix = c[len('label_llm'):].lstrip('_')\n",
        "                    norm_suffix = _normalize_token(suffix)\n",
        "                    if any(a in norm_suffix for a in norm_aliases):\n",
        "                        cols.append(c)\n",
        "                return cols\n",
        "\n",
        "            main_cols = _match_model_cols(df_plot_src, ['gpt-4o', 'gpt_4o', 'gpt4o'])\n",
        "            if len(main_cols) < 2:\n",
        "                raise ValueError(\"Need at least two 'gpt-4o' sampled columns in Combined_Files to compute FSD.\")\n",
        "\n",
        "            df_with_scores = calculate_fsd(df_plot_src, main_cols, return_row_scores=True, row_score_col='fsd')\n",
        "            fsd_df = pd.DataFrame({'fsd': df_with_scores['fsd']})\n",
        "            fsd_df.to_csv(fsd_out_path, index=False)\n",
        "            print(f\"Saved sampling-based FSD series to {fsd_out_path}\")\n",
        "\n",
        "            # Step B: Run FSD-gated threshold plot\n",
        "            summary_fsd = plot_kappa_vs_threshold_fearspeech_with_fsd(\n",
        "                    base_path=BASE_PATH,\n",
        "                    save_path=plot_save_fsd,\n",
        "                    main_aliases=['gpt_4o'],\n",
        "                    families_for_mv=['gpt_4_1_2025_04_14', 'o3_mini'],\n",
        "                    weighted_kappa=is_multi_label_task,\n",
        "            )\n",
        "            print(f\"FSD-gated threshold plot saved: {plot_save_fsd}\")\n",
        "            if summary_fsd is not None and not summary_fsd.empty:\n",
        "                print(\"Kappa vs FSD threshold:\")\n",
        "                display(summary_fsd)\n",
        "        else:\n",
        "            print(\"Warning: Insufficient data (ground truth or model columns) for threshold plot. Skipping.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Failed to generate threshold plot: {e}\")\n",
        "else:\n",
        "    print(\"Warning: Combined fearspeech CSV not found. Skipping threshold plot.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
