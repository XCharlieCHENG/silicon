import openai
import pandas as pd
from tqdm import tqdm
import os
import numpy as np
from datetime import datetime
import re
from google.genai import types


class Chatbot:
    """A chatbot that generates responses using one of OpenAI's GPT* language models."""

    def __init__(self, client, model='gpt-3.5-turbo', sys_prompt='You are a helpful assistant.', name='assistant', temperature=0, logprobs=True, top_logprobs='Null'):
        """
        Initializes the chatbot.

        Args:
        model (str): The name of the OpenAI language model to use.
        sys_prompt (str): A description of the chatbot (system prompt).
        name (str): The name of the chatbot.
        temperature (float): The randomness of the model's responses.
        """
        self.client = client
        self.history = [{"role": "system", "content": sys_prompt}]
        self.model = model
        self.name = name
        self.temperature = temperature
        self.logprobs = logprobs
        self.top_logprobs = top_logprobs

    def user_input(self, text):
        """
        Adds the user's input to the chatbot's history.

        Args:
        text (str): The user's input.
        """
        self.history.append({"role": "user", "content": text.strip()})

    def assistant_input(self, text):
        """
        Adds the assistant's input to the chatbot's history.

        Args:
        text (str): The assistant's input.
        """
        self.history.append({"role": "assistant", "content": text.strip()})

    def generate_response(self):
        """
        Generates a response using the chatbot's history and the OpenAI language model.

        Returns:
        str: The response generated by the chatbot.
        """
        try:
            if self.model in ['gpt-3.5-turbo', 'gpt-4', 'gpt-4-1106-preview', "gpt-4o", 'gpt-4.1-2025-04-14']:
                # For documentation on the function see
                # https://platform.openai.com/docs/guides/chat/introduction
                # https://platform.openai.com/docs/api-reference/chat

                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=self.history,
                    logprobs=self.logprobs,
                    top_logprobs=self.top_logprobs
                )
                response_text = response
            elif self.model in ['o3-mini', "gpt-4.5-preview"]:
                # Models that do not support logprobs.

                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=self.history,
                )
                response_text = response
            else:
                prompt = "\n".join([f"{x['role'].replace('assistant', self.name)}: {x['content']}" for x in self.history]) + f'\n{self.name}:'
                # For documentation on the function see
                # https://platform.openai.com/docs/api-reference/completions
                response = self.client.chat.completions.create(
                    model=self.model,
                    prompt=prompt,
                    temperature=0,
                    max_tokens=1000,
                    top_p=1,
                    frequency_penalty=0.0,
                    presence_penalty=0.0,
                    stop=["\n"]
                    )
                response_text = response["choices"][0]["text"].strip()
        except openai.OpenAIError as e:
            response_text = ''
            print(f'Error generating response: {e}')
        self.history.append({"role": "assistant", "content": response_text.choices[0].message.content})
        return response_text



def convert_json_to_text(read_text=True, file_path="", pattern=None):
    """
    Converts JSON-like text into a structured DataFrame based on the provided regex pattern.

    Args:
        read_text (bool): If True, reads from a file. If False, treats `file_path` as the text input.
        file_path (str): The file path or direct text input.
        pattern (str): A regex pattern to extract data from the text.

    Returns:
        pd.DataFrame: A DataFrame containing extracted values.
    """
    # Read text data if a file is provided
    if read_text:
        with open(file_path, 'r') as file:
            text_data = file.read()
    else:
        text_data = file_path

    # Ensure a pattern is provided; default to extracting "id" and "category" only
    if pattern is None:
        pattern = r'"id":\s*"([^"]+)"\s*,\s*"category":\s*"([^"]+)"'

    # ✅ Auto-detect if the pattern contains "explanation" and set `include_explanation` accordingly
    include_explanation = "explanation" in pattern

    # Extract matches using the regex pattern
    matches = re.findall(pattern, text_data)

    # Determine column names based on the regex groups
    column_names = ['id', 'category']  # Default names
    if include_explanation:
        column_names.insert(1, 'explanation')  # Insert "explanation" at the second position

    # Create DataFrame
    df = pd.DataFrame(matches, columns=column_names)

    return df



def extract_sum_logprobs(logprobs):
    """
    Extract the label and the sum of logprobs for the label from the given logprobs content.

    Parameters:
    - logprobs: List of ChatCompletionTokenLogprob objects containing token information

    Returns:
    - A tuple containing the extracted label and the sum of logprobs for the label
    """
    # Initialize variables to hold the start of label, the label itself, and the sum of logprobs
    sum_logprobs = 0
    # Iterate through the logprobs to extract the label and calculate the sum of logprobs
    for token in logprobs:
        sum_logprobs += token.logprob  # Add the logprob of the current token to the sum

    return sum_logprobs


def run_gpt_instruction_in_sys(client, item, model='gpt-4-1106-preview', instruction_prompt="You are a helpful assistant.", user_prompt_beginning=""):
    gb = Chatbot(name='Alice',
                client=client,
                model=model,
                sys_prompt=instruction_prompt,
                temperature = 1,
                logprobs = True,
                top_logprobs = 10) # https://platform.openai.com/docs/api-reference/chat/create#chat-create-top_logprobs
    prompt = user_prompt_beginning + item
    gb.user_input(prompt)
    response = gb.generate_response()

    response_text = response.choices[0].message.content

    # Check if logprobs exist in response
    if hasattr(response.choices[0], "logprobs") and response.choices[0].logprobs:
        response_logprobs = response.choices[0].logprobs.content
        sum_logprobs = extract_sum_logprobs(response_logprobs)
    else:
        response_logprobs = np.nan # NaN value
        sum_logprobs = np.nan  # NaN value

    return response_text, response_logprobs, response.usage.completion_tokens, response.usage.prompt_tokens, sum_logprobs


def run_deepseek(client, model, system_prompt, user_prompt_beginning, text):
    user_prompt = user_prompt_beginning + text
    if model == "deepseek-chat":
        response = client.chat.completions.create(
                model=model,
                messages=[
                      {"role": "system", "content": system_prompt},
                      {"role": "user", "content": user_prompt}],
                max_tokens=None,
                temperature=1,
                top_p=1,
                logprobs=True,
                top_logprobs=10,
                # top_k=50,
            )
        return response.choices[0].message.content, response.choices[0].logprobs, response.usage.completion_tokens, response.usage.prompt_tokens
    else:
        response = client.chat.completions.create(
                model=model,
                messages=[
                      {"role": "system", "content": system_prompt},
                      {"role": "user", "content": user_prompt}],
                max_tokens=None,
                temperature=1,
                top_p=1,
                logprobs=False
            )
        return response.choices[0].message.content, None, response.usage.completion_tokens, response.usage.prompt_tokens


def run_gemini(client, model, system_prompt, user_prompt_beginning, text):
    user_prompt = user_prompt_beginning + text
    if model == "gemini-1.5-pro-latest":
        response = client.models.generate_content(
                    model=model,
                    config=types.GenerateContentConfig(
                                                    temperature=1,
                                                    top_p=1,
                                                    system_instruction=system_prompt,
                                                    response_logprobs=False
                                                    ),
                     contents=user_prompt
                )
        return response.text, None
    if model == "gemini-2.5-pro":
        response = client.models.generate_content(
                    model=model,
                    config=types.GenerateContentConfig(
                                                    temperature=1,
                                                    top_p=1,
                                                    system_instruction=system_prompt,
                                                    response_logprobs=True,
                                                    logprobs=5,
                                                    ),
                    contents=user_prompt,
                )
        return response.text, response.candidates[0].logprobs_result


def run_claude(client, model, system_prompt, user_prompt_beginning, text):
    user_prompt = user_prompt_beginning + text

    message = client.messages.create(
        model=model, 
        max_tokens=8192,
        temperature = 1,
        system=system_prompt,
        messages=[
            {"role": "user", "content": user_prompt}
        ]
    )
    return message.content[0].text, message.usage.output_tokens, message.usage.input_tokens


def obtain_llm_results(client, df, model, instruction_prompt, user_prompt_beginning, model_series='gpt'):
    results = {
        'text': [],
        'logprobs': [],
        'sum_logprobs': [],
        'token_completion': [],
        'token_prompt': []
    }
    
    for text in tqdm(df['input'].tolist()):
        try:
            if model_series == "gpt":
                text, logprobs, token_completion, token_prompt, sum_logprobs = run_gpt_instruction_in_sys(
                    client, text, model=model,
                    instruction_prompt=instruction_prompt,
                    user_prompt_beginning=user_prompt_beginning
                )
            elif model_series == "deepseek":
                text, logprobs, token_completion, token_prompt = run_deepseek(
                    client, model=model, system_prompt=instruction_prompt, user_prompt_beginning=user_prompt_beginning, text=text
                )
                sum_logprobs = None  # Deepseek chat does not return sum_logprobs
            elif model_series == "gemini":
                text, logprobs = run_gemini(
                    client, model=model, system_prompt=instruction_prompt, user_prompt_beginning=user_prompt_beginning, text=text
                )
                token_completion = None
                token_prompt = None
                sum_logprobs = None  
            elif model_series == "claude":
                text, token_completion, token_prompt = run_claude(
                    client, model=model, system_prompt=instruction_prompt, user_prompt_beginning=user_prompt_beginning, text=text
                )
                logprobs = None  
                sum_logprobs = None  
            else:
                raise ValueError(f"Unsupported model_series: {model_series}")
            
            results['text'].append(text)
            results['logprobs'].append(logprobs)
            results['sum_logprobs'].append(sum_logprobs)
            results['token_completion'].append(token_completion)
            results['token_prompt'].append(token_prompt)
        except Exception as e:
            print(f"An error occurred: {e}")
            break
    
    return pd.DataFrame(results)


def run_llm(client, df, model, instruction_prompt, user_prompt_beginning, base_path, pattern, model_series):
    def get_llm_input(df):
        inputs = df.apply(lambda row: f"""The id is "{row['id']}". Here is the content: {row['text']}""", axis=1)
        return pd.DataFrame(inputs, columns=['input'])
    llm_input_df = get_llm_input(df)

    start_time = datetime.now()
    print(f"Total number of items: {len(df)}")

    # Run and Store LLM outputs
    df_llm = obtain_llm_results(client, llm_input_df, model, instruction_prompt, user_prompt_beginning, model_series)

    # Save Temp LLM results inside silicon_demo/wip
    import os as _os
    _os.makedirs(_os.path.join(base_path, 'wip'), exist_ok=True)
    # Prefer a filename-safe token: support both hyphen and underscore variants for backward compatibility
    filename_safe = model.replace('.', '_')  # hyphen kept as-is
    filename_safe_alt = filename_safe.replace('-', '_')  # underscore variant used by some prior runs
    tmp_path = _os.path.join(base_path, 'wip', f'fearspeech_{filename_safe}.csv')
    df_llm.to_csv(tmp_path, index=False)

    # Calculate and print processing time and token statistics
    duration = datetime.now() - start_time
    print(f'Running time: {duration}')
    print(f'Completion token count: {df_llm.token_completion.sum()}.')
    print(f'Prompt token count: {df_llm.token_prompt.sum()}.')

    # Convert JSON text to structured format
    df_llm = convert_json_to_text(read_text=False, file_path="\n".join(df_llm["text"].tolist()), pattern=pattern)

    # Load the original dataset
    df_full = df.copy()

    # Ensure 'id' columns are of type string for merging
    df_full['id'] = df_full['id'].astype(str)
    df_llm['id'] = df_llm['id'].astype(str)

    # Rename the category column and merge with the original dataset
    df_llm = df_llm.rename(columns={'category': 'label_llm'})
    df_full = df_full.merge(df_llm, on=['id'], how='left')

    # Save final annotated dataset inside silicon_demo/outputs/fearspeech
    _os.makedirs(_os.path.join(base_path, 'outputs', 'fearspeech'), exist_ok=True)
    out_dir = _os.path.join(base_path, 'outputs', 'fearspeech')
    base_name = f"iteration_{filename_safe}.csv"
    candidate_path = _os.path.join(out_dir, base_name)

    # Determine next available repeat index by scanning existing files (both hyphen and underscore variants)
    import re as _re
    entries = [f for f in _os.listdir(out_dir) if f.endswith('.csv')]
    # Patterns for base and repeats
    pat_any = _re.compile(rf"^iteration_(?:{_re.escape(filename_safe)}|{_re.escape(filename_safe_alt)})(?:_repeat(\\d+))?\\.csv$")
    pat_base_hyphen = f"iteration_{filename_safe}.csv"
    pat_base_underscore = f"iteration_{filename_safe_alt}.csv"

    repeat_indices = []  # collect numeric repeat suffixes
    base_exists = False
    matched_variant = None  # which variant (hyphen/underscore) is present in existing files

    for fname in entries:
        # Track existence of base files
        if fname == pat_base_hyphen:
            base_exists = True
            matched_variant = matched_variant or filename_safe
        if fname == pat_base_underscore:
            base_exists = True
            matched_variant = matched_variant or filename_safe_alt

        # Track repeats and remember variant
        m = pat_any.match(fname)
        if m:
            if matched_variant is None:
                matched_variant = filename_safe_alt if f"iteration_{filename_safe_alt}" in fname else filename_safe
            if m.group(1) is not None:
                try:
                    repeat_indices.append(int(m.group(1)))
                except Exception:
                    pass

    if not base_exists and not repeat_indices:
        # Nothing exists yet → create base with hyphen-preferred variant
        final_name = base_name
    else:
        # Either base exists or repeats exist → compute next repeat
        if repeat_indices:
            next_idx = max(repeat_indices) + 1
        else:
            # base exists but no repeats yet
            next_idx = 1
        # Respect existing variant if any; default to hyphen variant
        chosen_safe = matched_variant or filename_safe
        final_name = f"iteration_{chosen_safe}_repeat{next_idx}.csv"

    final_output_path = _os.path.join(out_dir, final_name)
    # Safeguard: if the computed name somehow exists, increment repeat index until free
    # This handles potential race conditions or legacy files slipping through the scan.
    if _os.path.exists(final_output_path):
        prefix = final_name.split('_repeat')[0] if '_repeat' in final_name else final_name[:-4]
        # Determine starting index
        m_idx = _re.search(r"_repeat(\d+)\.csv$", final_name)
        next_idx = int(m_idx.group(1)) if m_idx else 1
        while _os.path.exists(final_output_path):
            next_idx += 1
            if prefix.endswith('.csv'):
                # Should not happen, but guard
                prefix = prefix[:-4]
            final_name = f"{prefix}_repeat{next_idx}.csv"
            final_output_path = _os.path.join(out_dir, final_name)
    df_full.to_csv(final_output_path, index=False)

    print(f"Final annotation file saved at: {final_output_path}")
    return df_full


def merge_multiple_runs(
    base_path: str,
    relative_path: str,
    file_pattern: str,
    iteration_cols: list,
    key: str,
    output_dir: str,
    keep_cols: list,
):
    """Merge multiple iteration CSVs for a task into a single Combined_Files CSV.

    - Looks for files under join(base_path, relative_path) matching file_pattern (regex).
    - For each matched file, selects iteration_cols and renames them with a unique suffix.
    - Merges on 'id' and retains keep_cols plus the per-iteration columns.
    - Writes to join(base_path, output_dir, f"{key}.csv").

    If no files found, attempts to return the existing combined CSV if present.
    """
    import re as _re
    import os as _os
    import pandas as _pd

    src_dir = _os.path.join(base_path, relative_path)
    combined_dir = _os.path.join(base_path, output_dir)
    combined_path = _os.path.join(combined_dir, f"{key}.csv")

    if not _os.path.isdir(src_dir):
        # Ensure source directory exists; gracefully fall back
        _os.makedirs(src_dir, exist_ok=True)
        if _os.path.exists(combined_path):
            return _pd.read_csv(combined_path)
        return _pd.DataFrame(columns=keep_cols)

    # Enumerate candidate files
    try:
        entries = sorted(
            [f for f in _os.listdir(src_dir) if f.endswith('.csv')]
        )
    except Exception as e:
        # Fallback: if directory listing fails, try returning existing combined
        if _os.path.exists(combined_path):
            return _pd.read_csv(combined_path)
        raise e

    # Filter by regex and extract iteration index
    candidates = []
    pattern = _re.compile(file_pattern)
    for fname in entries:
        m = pattern.search(fname)
        if m:
            try:
                iteration_idx = int(m.group(1)) if m.group(1).isdigit() else m.group(1)
            except Exception:
                iteration_idx = m.group(1)
            candidates.append((iteration_idx, fname))

    if not candidates:
        if _os.path.exists(combined_path):
            return _pd.read_csv(combined_path)
        # Nothing to merge; return empty DataFrame with keep_cols
        return _pd.DataFrame(columns=keep_cols)

    # Sort by iteration index when numeric
    try:
        candidates.sort(key=lambda x: int(x[0]))
    except Exception:
        candidates.sort(key=lambda x: str(x[0]))

    merged_df = None
    for idx, fname in candidates:
        fpath = _os.path.join(src_dir, fname)
        df_iter = _pd.read_csv(fpath)

        # Ensure id is string
        if 'id' in df_iter.columns:
            df_iter['id'] = df_iter['id'].astype(str)

        # Keep only necessary columns
        select_cols = ['id'] + [c for c in iteration_cols if c in df_iter.columns]
        df_iter = df_iter[select_cols].copy()

        # Rename iteration columns with suffix
        rename_map = {}
        for c in iteration_cols:
            if c in df_iter.columns:
                rename_map[c] = f"{c}{idx}"
        df_iter.rename(columns=rename_map, inplace=True)

        if merged_df is None:
            merged_df = df_iter
        else:
            merged_df = merged_df.merge(df_iter, on='id', how='outer')

    # Attach kept columns if available from the latest file having them
    if merged_df is None:
        if _os.path.exists(combined_path):
            return _pd.read_csv(combined_path)
        return _pd.DataFrame(columns=keep_cols)

    # Try to enrich with keep_cols from any source file
    for _, fname in reversed(candidates):
        fpath = _os.path.join(src_dir, fname)
        df_full = _pd.read_csv(fpath)
        df_full['id'] = df_full['id'].astype(str) if 'id' in df_full.columns else df_full['id']
        present = [c for c in keep_cols if c in df_full.columns]
        present_no_id = [c for c in present if c != 'id']
        if present_no_id:
            merged_df = merged_df.merge(df_full[['id'] + present_no_id].drop_duplicates('id'), on='id', how='left')
            break

    # Reorder columns: keep_cols first (when present), then iteration columns
    keep_present = [c for c in keep_cols if c in merged_df.columns]
    iter_present = [c for c in merged_df.columns if c not in keep_present and c != 'id']
    ordered_cols = ['id'] + keep_present + iter_present
    merged_df = merged_df[ordered_cols]

    _os.makedirs(combined_dir, exist_ok=True)
    merged_df.to_csv(combined_path, index=False)
    return merged_df

